{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Retrain the YOLO model for your own dataset.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\n",
    "from yolo3.utils import get_random_data\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "def get_classes(classes_path):\n",
    "    '''loads the classes'''\n",
    "    with open(classes_path) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "    return class_names\n",
    "\n",
    "def get_anchors(anchors_path):\n",
    "    '''loads the anchors from a file'''\n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "\n",
    "def create_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
    "            weights_path='model_data/yolo_weights.h5'):\n",
    "    '''create the training model'''\n",
    "    K.clear_session() # get a new session\n",
    "    image_input = Input(shape=(None, None, 3))\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\n",
    "        num_anchors//3, num_classes+5)) for l in range(3)]\n",
    "\n",
    "    model_body = yolo_body(image_input, num_anchors//3, num_classes)\n",
    "    print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    if load_pretrained:\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2]:\n",
    "            # Freeze darknet53 body or freeze all but 3 output layers.\n",
    "            num = (185, len(model_body.layers)-3)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})(\n",
    "        [*model_body.output, *y_true])\n",
    "    model = Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_tiny_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
    "            weights_path='model_data/tiny_yolo_weights.h5'):\n",
    "    '''create the training model, for Tiny YOLOv3'''\n",
    "    K.clear_session() # get a new session\n",
    "    image_input = Input(shape=(None, None, 3))\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16}[l], w//{0:32, 1:16}[l], \\\n",
    "        num_anchors//2, num_classes+5)) for l in range(2)]\n",
    "\n",
    "    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n",
    "    print('Create Tiny YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    if load_pretrained:\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2]:\n",
    "            # Freeze the darknet body or freeze all but 2 output layers.\n",
    "            num = (20, len(model_body.layers)-2)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.7})(\n",
    "        [*model_body.output, *y_true])\n",
    "    model = Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    '''data generator for fit_generator'''\n",
    "    n = len(annotation_lines)\n",
    "    i = 0\n",
    "    while True:\n",
    "        image_data = []\n",
    "        box_data = []\n",
    "        for b in range(batch_size):\n",
    "            if i==0:\n",
    "                np.random.shuffle(annotation_lines)\n",
    "            image, box = get_random_data(annotation_lines[i], input_shape, random=True)\n",
    "            image_data.append(image)\n",
    "            box_data.append(box)\n",
    "            i = (i+1) % n\n",
    "        image_data = np.array(image_data)\n",
    "        box_data = np.array(box_data)\n",
    "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
    "        yield [image_data, *y_true], np.zeros(batch_size)\n",
    "\n",
    "def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    n = len(annotation_lines)\n",
    "    if n==0 or batch_size<=0: return None\n",
    "    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_path = 'train.txt'\n",
    "log_dir = 'logs/svhn_weights/'\n",
    "classes_path = 'model_data/svhn_classes.txt'\n",
    "anchors_path = 'model_data/yolo_anchors.txt'\n",
    "class_names = get_classes(classes_path)\n",
    "num_classes = len(class_names)\n",
    "anchors = get_anchors(anchors_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVHN Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 0.1\n",
    "\n",
    "with open(annotation_path) as f:\n",
    "    lines = f.readlines()\n",
    "np.random.seed(10101)\n",
    "np.random.shuffle(lines)\n",
    "np.random.seed(None)\n",
    "num_val = int(len(lines)*val_split)\n",
    "num_train = len(lines) - num_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging = TensorBoard(log_dir=log_dir)\n",
    "checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
    "    monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Weights Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "Create YOLOv3 model with 9 anchors and 10 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer conv2d_56 due to mismatch in shape ((3, 3, 512, 1024) vs (512, 2048, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer batch_normalization_56 due to mismatch in shape ((1024,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer conv2d_57 due to mismatch in shape ((1, 1, 1024, 512) vs (1024, 512, 3, 3)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer batch_normalization_57 due to mismatch in shape ((512,) vs (1024,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer conv2d_58 due to mismatch in shape ((3, 3, 512, 1024) vs (512, 1024, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer batch_normalization_58 due to mismatch in shape ((1024,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer conv2d_61 due to mismatch in shape ((1, 1, 768, 256) vs (256, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer conv2d_62 due to mismatch in shape ((3, 3, 256, 512) vs (256, 768, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer batch_normalization_61 due to mismatch in shape ((512,) vs (256,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer conv2d_63 due to mismatch in shape ((1, 1, 512, 256) vs (512, 256, 3, 3)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer batch_normalization_62 due to mismatch in shape ((256,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer conv2d_64 due to mismatch in shape ((3, 3, 256, 512) vs (256, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer batch_normalization_63 due to mismatch in shape ((512,) vs (256,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer conv2d_65 due to mismatch in shape ((1, 1, 512, 256) vs (512, 256, 3, 3)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer batch_normalization_64 due to mismatch in shape ((256,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer conv2d_66 due to mismatch in shape ((3, 3, 256, 512) vs (256, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer batch_normalization_65 due to mismatch in shape ((512,) vs (256,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer conv2d_69 due to mismatch in shape ((1, 1, 384, 128) vs (128, 256, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer conv2d_70 due to mismatch in shape ((3, 3, 128, 256) vs (128, 384, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer batch_normalization_68 due to mismatch in shape ((256,) vs (128,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer conv2d_71 due to mismatch in shape ((1, 1, 256, 128) vs (256, 128, 3, 3)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer batch_normalization_69 due to mismatch in shape ((128,) vs (256,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer conv2d_72 due to mismatch in shape ((3, 3, 128, 256) vs (128, 256, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer batch_normalization_70 due to mismatch in shape ((256,) vs (128,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer conv2d_73 due to mismatch in shape ((1, 1, 256, 128) vs (256, 128, 3, 3)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer batch_normalization_71 due to mismatch in shape ((128,) vs (256,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer conv2d_74 due to mismatch in shape ((3, 3, 128, 256) vs (128, 256, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer batch_normalization_72 due to mismatch in shape ((256,) vs (128,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1262: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in number of weights (2 vs 1).\n",
      "  len(symbolic_weights), len(weight_values)))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1262: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in number of weights (2 vs 1).\n",
      "  len(symbolic_weights), len(weight_values)))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1262: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in number of weights (2 vs 1).\n",
      "  len(symbolic_weights), len(weight_values)))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer batch_normalization_59 due to mismatch in shape ((256,) vs (1024,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1281: UserWarning: Skipping loading of weights for layer batch_normalization_66 due to mismatch in shape ((128,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1262: UserWarning: Skipping loading of weights for layer conv2d_60 due to mismatch in number of weights (1 vs 2).\n",
      "  len(symbolic_weights), len(weight_values)))\n",
      "C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\engine\\saving.py:1262: UserWarning: Skipping loading of weights for layer conv2d_68 due to mismatch in number of weights (1 vs 2).\n",
      "  len(symbolic_weights), len(weight_values)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weights model_data/yolov3-spp_weights.h5.\n",
      "Freeze the first 249 layers of total 252 layers.\n",
      "WARNING:tensorflow:From C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3351: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 3 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 3 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, None, None, 3 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, None, None, 3 0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 6 18432       zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 6 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 3 2048        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 3 128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, None, None, 3 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 6 18432       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, None, 6 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, None, 6 0           leaky_re_lu_2[0][0]              \n",
      "                                                                 leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, None, None, 6 0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 1 73728       zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, None, 1 512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 6 8192        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, None, 6 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 1 73728       leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, None, 1 512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, None, None, 1 0           leaky_re_lu_5[0][0]              \n",
      "                                                                 leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 6 8192        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, None, 6 256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 1 73728       leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, None, 1 512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, None, None, 1 0           add_2[0][0]                      \n",
      "                                                                 leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, None, None, 1 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 2 294912      zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, None, 2 1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, None, 1 512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, None, 2 1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, None, None, 2 0           leaky_re_lu_10[0][0]             \n",
      "                                                                 leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 1 32768       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, None, 1 512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, None, 2 1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, None, None, 2 0           add_4[0][0]                      \n",
      "                                                                 leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, None, None, 1 32768       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, None, 1 512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, None, 2 1024        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, None, None, 2 0           add_5[0][0]                      \n",
      "                                                                 leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, None, None, 1 32768       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, None, 1 512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, None, 2 1024        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, None, None, 2 0           add_6[0][0]                      \n",
      "                                                                 leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, None, None, 1 32768       add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, None, 1 512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, None, 2 1024        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, None, None, 2 0           add_7[0][0]                      \n",
      "                                                                 leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, None, None, 1 32768       add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, None, None, 1 512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, None, None, 2 1024        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, None, None, 2 0           add_8[0][0]                      \n",
      "                                                                 leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, None, None, 1 32768       add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, None, None, 1 512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, None, None, 2 1024        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, None, None, 2 0           add_9[0][0]                      \n",
      "                                                                 leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, None, None, 1 32768       add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, None, None, 1 512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, None, None, 2 1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, None, None, 2 0           add_10[0][0]                     \n",
      "                                                                 leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, None, None, 2 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, None, None, 5 1179648     zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, None, None, 5 2048        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, None, None, 2 131072      leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, None, None, 2 1024        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, None, None, 5 2048        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, None, None, 5 0           leaky_re_lu_27[0][0]             \n",
      "                                                                 leaky_re_lu_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, None, None, 2 131072      add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, None, None, 2 1024        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, None, None, 5 2048        conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, None, None, 5 0           add_12[0][0]                     \n",
      "                                                                 leaky_re_lu_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, None, None, 2 131072      add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, None, None, 2 1024        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, None, None, 5 2048        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, None, None, 5 0           add_13[0][0]                     \n",
      "                                                                 leaky_re_lu_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, None, None, 2 131072      add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, None, None, 2 1024        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, None, None, 5 2048        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, None, None, 5 0           add_14[0][0]                     \n",
      "                                                                 leaky_re_lu_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, None, None, 2 131072      add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, None, None, 2 1024        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, None, None, 5 2048        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, None, None, 5 0           add_15[0][0]                     \n",
      "                                                                 leaky_re_lu_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, None, None, 2 131072      add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, None, None, 2 1024        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, None, None, 5 2048        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, None, None, 5 0           add_16[0][0]                     \n",
      "                                                                 leaky_re_lu_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, None, None, 2 131072      add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, None, None, 2 1024        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, None, None, 5 2048        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, None, None, 5 0           add_17[0][0]                     \n",
      "                                                                 leaky_re_lu_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, None, None, 2 131072      add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, None, None, 2 1024        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, None, None, 5 2048        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, None, None, 5 0           add_18[0][0]                     \n",
      "                                                                 leaky_re_lu_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, None, None, 5 0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, None, None, 1 4718592     zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, None, None, 1 4096        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, None, None, 5 524288      leaky_re_lu_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, None, None, 5 2048        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, None, None, 1 4096        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, None, None, 1 0           leaky_re_lu_44[0][0]             \n",
      "                                                                 leaky_re_lu_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, None, None, 5 524288      add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, None, None, 5 2048        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, None, None, 1 4096        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, None, None, 1 0           add_20[0][0]                     \n",
      "                                                                 leaky_re_lu_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, None, None, 5 524288      add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, None, None, 5 2048        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, None, None, 1 4096        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, None, None, 1 0           add_21[0][0]                     \n",
      "                                                                 leaky_re_lu_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, None, None, 5 524288      add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, None, None, 5 2048        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, None, None, 1 4096        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_52 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, None, None, 1 0           add_22[0][0]                     \n",
      "                                                                 leaky_re_lu_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, None, None, 5 524288      add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, None, None, 5 2048        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_53 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, None, None, 1 4096        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_54 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, None, None, 5 524288      leaky_re_lu_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, None, None, 5 2048        conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_55 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, None, None, 1 4096        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_56 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, None, None, 5 524288      leaky_re_lu_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, None, None, 5 2048        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_57 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, None, None, 2 131072      leaky_re_lu_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, None, None, 2 1024        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_59 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, None, None, 2 0           leaky_re_lu_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 7 0           up_sampling2d_1[0][0]            \n",
      "                                                                 add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, None, None, 2 196608      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, None, None, 2 1024        conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_60 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, None, None, 5 2048        conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_61 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, None, None, 2 131072      leaky_re_lu_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, None, None, 2 1024        conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_62 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, None, None, 5 2048        conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_63 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, None, None, 2 131072      leaky_re_lu_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, None, None, 2 1024        conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_64 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, None, None, 1 512         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_66 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, None, None, 1 0           leaky_re_lu_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, None, 3 0           up_sampling2d_2[0][0]            \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, None, None, 1 49152       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, None, None, 1 512         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, None, None, 2 1024        conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, None, None, 1 512         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, None, None, 2 1024        conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_70 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, None, None, 1 512         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, None, None, 1 4096        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, None, None, 5 2048        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, None, None, 2 1024        conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_58 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, None, None, 4 46125       leaky_re_lu_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, None, None, 4 23085       leaky_re_lu_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, None, None, 4 11565       leaky_re_lu_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 13, 13, 3, 15 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 26, 26, 3, 15 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 52, 52, 3, 15 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_loss (Lambda)              (None, 1)            0           conv2d_59[0][0]                  \n",
      "                                                                 conv2d_67[0][0]                  \n",
      "                                                                 conv2d_75[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 61,624,807\n",
      "Trainable params: 80,775\n",
      "Non-trainable params: 61,544,032\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (416,416) # multiple of 32, hw\n",
    "\n",
    "is_tiny_version = len(anchors)==6 # default setting\n",
    "if is_tiny_version:\n",
    "    model = create_tiny_model(input_shape, anchors, num_classes,\n",
    "        freeze_body=2, weights_path='yolov3-tiny-weights/yolov3-tiny_weights.h5')\n",
    "else:\n",
    "    model = create_model(input_shape, anchors, num_classes,\n",
    "        freeze_body=2, weights_path='yolov3-spp-weights/yolov3-spp_weights.h5') # make sure you know what you freeze\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimizer:Adam\n",
    "* Learning Rate: 1e-3\n",
    "* Loss: yolo loss\n",
    "* Batch size: 32\n",
    "* Epoch: 50   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Train on 30062 samples, val on 3340 samples, with batch size 4.\n",
      "WARNING:tensorflow:From C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\YanRoo\\Anaconda3\\envs\\Keras\\lib\\site-packages\\keras\\callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/50\n",
      "7515/7515 [==============================] - 3407s 453ms/step - loss: 62.7420 - val_loss: 1720.7758\n",
      "Epoch 2/50\n",
      "7515/7515 [==============================] - 3240s 431ms/step - loss: 33.3764 - val_loss: 4269.7632\n",
      "Epoch 3/50\n",
      "7515/7515 [==============================] - 3235s 431ms/step - loss: 33.1643 - val_loss: 3002.1499\n",
      "Epoch 4/50\n",
      "7515/7515 [==============================] - 3218s 428ms/step - loss: 33.1262 - val_loss: 2461.2789\n",
      "Epoch 5/50\n",
      "7515/7515 [==============================] - 3253s 433ms/step - loss: 33.1062 - val_loss: 1626.3356\n",
      "Epoch 6/50\n",
      "7515/7515 [==============================] - 3243s 432ms/step - loss: 33.0667 - val_loss: 1198.3815\n",
      "Epoch 7/50\n",
      "7515/7515 [==============================] - 3211s 427ms/step - loss: 33.0275 - val_loss: 1391.7292\n",
      "Epoch 8/50\n",
      "7515/7515 [==============================] - 3215s 428ms/step - loss: 32.9765 - val_loss: 861.4954\n",
      "Epoch 9/50\n",
      "7515/7515 [==============================] - 3212s 427ms/step - loss: 33.0162 - val_loss: 933.9990\n",
      "Epoch 10/50\n",
      "7515/7515 [==============================] - 3242s 431ms/step - loss: 32.9854 - val_loss: 926.8864\n",
      "Epoch 11/50\n",
      "7515/7515 [==============================] - 3215s 428ms/step - loss: 32.9509 - val_loss: 602.3885\n",
      "Epoch 12/50\n",
      "7515/7515 [==============================] - 3207s 427ms/step - loss: 32.9204 - val_loss: 591.7518\n",
      "Epoch 13/50\n",
      "7515/7515 [==============================] - 3217s 428ms/step - loss: 32.8992 - val_loss: 462.4062\n",
      "Epoch 14/50\n",
      "7515/7515 [==============================] - 3212s 427ms/step - loss: 32.9257 - val_loss: 460.0448\n",
      "Epoch 15/50\n",
      "7515/7515 [==============================] - 3214s 428ms/step - loss: 32.9340 - val_loss: 378.9400\n",
      "Epoch 16/50\n",
      "7515/7515 [==============================] - 3214s 428ms/step - loss: 32.9145 - val_loss: 313.1661\n",
      "Epoch 17/50\n",
      "7515/7515 [==============================] - 3211s 427ms/step - loss: 32.9112 - val_loss: 329.9374\n",
      "Epoch 18/50\n",
      "7515/7515 [==============================] - 3212s 427ms/step - loss: 32.8910 - val_loss: 296.2050\n",
      "Epoch 19/50\n",
      "7515/7515 [==============================] - 3218s 428ms/step - loss: 32.9335 - val_loss: 347.3313\n",
      "Epoch 20/50\n",
      "7515/7515 [==============================] - 3212s 427ms/step - loss: 32.8870 - val_loss: 352.5412\n",
      "Epoch 21/50\n",
      "7515/7515 [==============================] - 3213s 428ms/step - loss: 32.8785 - val_loss: 244.5285\n",
      "Epoch 22/50\n",
      "7515/7515 [==============================] - 3221s 429ms/step - loss: 32.8329 - val_loss: 213.8587\n",
      "Epoch 23/50\n",
      "7515/7515 [==============================] - 3226s 429ms/step - loss: 32.9103 - val_loss: 215.9086\n",
      "Epoch 24/50\n",
      "7515/7515 [==============================] - 3216s 428ms/step - loss: 32.9070 - val_loss: 197.2677\n",
      "Epoch 25/50\n",
      "7515/7515 [==============================] - 3605s 480ms/step - loss: 32.8206 - val_loss: 192.2005\n",
      "Epoch 26/50\n",
      "7515/7515 [==============================] - 3268s 435ms/step - loss: 32.8405 - val_loss: 162.0666\n",
      "Epoch 27/50\n",
      "7515/7515 [==============================] - 3220s 428ms/step - loss: 32.8364 - val_loss: 152.0361\n",
      "Epoch 28/50\n",
      "7515/7515 [==============================] - 3220s 429ms/step - loss: 32.8720 - val_loss: 154.8768\n",
      "Epoch 29/50\n",
      "7515/7515 [==============================] - 3266s 435ms/step - loss: 32.8401 - val_loss: 164.8877\n",
      "Epoch 30/50\n",
      "7515/7515 [==============================] - 3243s 432ms/step - loss: 32.8513 - val_loss: 143.4785\n",
      "Epoch 31/50\n",
      "7515/7515 [==============================] - 3224s 429ms/step - loss: 32.8368 - val_loss: 138.7946\n",
      "Epoch 32/50\n",
      "7515/7515 [==============================] - 3276s 436ms/step - loss: 32.8134 - val_loss: 151.9226\n",
      "Epoch 33/50\n",
      "7515/7515 [==============================] - 3240s 431ms/step - loss: 32.8207 - val_loss: 148.5220\n",
      "Epoch 34/50\n",
      "7515/7515 [==============================] - 3236s 431ms/step - loss: 32.8657 - val_loss: 137.6815\n",
      "Epoch 35/50\n",
      "7515/7515 [==============================] - 3236s 431ms/step - loss: 32.8383 - val_loss: 143.1043\n",
      "Epoch 36/50\n",
      "7515/7515 [==============================] - 3233s 430ms/step - loss: 32.8367 - val_loss: 132.7305\n",
      "Epoch 37/50\n",
      "7515/7515 [==============================] - 3251s 433ms/step - loss: 32.8258 - val_loss: 133.0648\n",
      "Epoch 38/50\n",
      "7515/7515 [==============================] - 3230s 430ms/step - loss: 32.8607 - val_loss: 139.5009\n",
      "Epoch 39/50\n",
      "7515/7515 [==============================] - 3230s 430ms/step - loss: 32.8447 - val_loss: 140.3994\n",
      "Epoch 40/50\n",
      "7515/7515 [==============================] - 3229s 430ms/step - loss: 32.8473 - val_loss: 120.1622\n",
      "Epoch 41/50\n",
      "7515/7515 [==============================] - 3267s 435ms/step - loss: 32.8369 - val_loss: 136.9639\n",
      "Epoch 42/50\n",
      "7515/7515 [==============================] - 3227s 429ms/step - loss: 32.8571 - val_loss: 124.3915\n",
      "Epoch 43/50\n",
      "7515/7515 [==============================] - 3255s 433ms/step - loss: 32.8414 - val_loss: 128.2825\n",
      "Epoch 44/50\n",
      "7515/7515 [==============================] - 3230s 430ms/step - loss: 32.8624 - val_loss: 131.7165\n",
      "Epoch 45/50\n",
      "7515/7515 [==============================] - 3232s 430ms/step - loss: 32.8664 - val_loss: 122.8212\n",
      "Epoch 46/50\n",
      "7515/7515 [==============================] - 3235s 430ms/step - loss: 32.8300 - val_loss: 106.3520\n",
      "Epoch 47/50\n",
      "7515/7515 [==============================] - 3231s 430ms/step - loss: 32.8334 - val_loss: 113.6686\n",
      "Epoch 48/50\n",
      "7515/7515 [==============================] - 3223s 429ms/step - loss: 32.8455 - val_loss: 120.7621\n",
      "Epoch 49/50\n",
      "7515/7515 [==============================] - 3242s 431ms/step - loss: 32.8427 - val_loss: 117.7575\n",
      "Epoch 50/50\n",
      "7515/7515 [==============================] - 3234s 430ms/step - loss: 32.8402 - val_loss: 115.4153\n"
     ]
    }
   ],
   "source": [
    "# Train with frozen layers first, to get a stable loss.\n",
    "# Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n",
    "if True:\n",
    "    model.compile(optimizer=Adam(lr=1e-3), loss={\n",
    "        # use custom yolo_loss Lambda layer.\n",
    "        'yolo_loss': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "    batch_size = 4\n",
    "    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "    model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "            steps_per_epoch=max(1, num_train//batch_size),\n",
    "            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "            validation_steps=max(1, num_val//batch_size),\n",
    "            epochs=50,\n",
    "            initial_epoch=0,\n",
    "            callbacks=[logging, checkpoint])\n",
    "    model.save_weights(log_dir + 'trained_weights_stage_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreeze all of the layers.\n",
      "Train on 30062 samples, val on 3340 samples, with batch size 4.\n",
      "Epoch 51/100\n",
      "7515/7515 [==============================] - 4571s 608ms/step - loss: 23.2992 - val_loss: 20.7675\n",
      "Epoch 52/100\n",
      "7515/7515 [==============================] - 4834s 643ms/step - loss: 19.1309 - val_loss: 18.1922\n",
      "Epoch 53/100\n",
      "7515/7515 [==============================] - 5815s 774ms/step - loss: 17.1317 - val_loss: 16.4351\n",
      "Epoch 54/100\n",
      "7515/7515 [==============================] - 4562s 607ms/step - loss: 15.9499 - val_loss: 15.9573\n",
      "Epoch 55/100\n",
      "7515/7515 [==============================] - 4538s 604ms/step - loss: 15.1846 - val_loss: 14.7872\n",
      "Epoch 56/100\n",
      "7515/7515 [==============================] - 4536s 604ms/step - loss: 14.5523 - val_loss: 14.2764\n",
      "Epoch 57/100\n",
      "7515/7515 [==============================] - 4536s 604ms/step - loss: 14.1254 - val_loss: 13.9959\n",
      "Epoch 58/100\n",
      "7515/7515 [==============================] - 4539s 604ms/step - loss: 13.8219 - val_loss: 13.7536\n",
      "Epoch 59/100\n",
      "7515/7515 [==============================] - 4537s 604ms/step - loss: 13.5221 - val_loss: 13.6216\n",
      "Epoch 60/100\n",
      "7515/7515 [==============================] - 4545s 605ms/step - loss: 13.3328 - val_loss: 13.4465\n",
      "Epoch 61/100\n",
      "7515/7515 [==============================] - 4537s 604ms/step - loss: 13.1081 - val_loss: 13.5780\n",
      "Epoch 62/100\n",
      "7515/7515 [==============================] - 4537s 604ms/step - loss: 12.9407 - val_loss: 12.9254\n",
      "Epoch 63/100\n",
      "7515/7515 [==============================] - 4523s 602ms/step - loss: 12.8310 - val_loss: 12.8418\n",
      "Epoch 64/100\n",
      "7515/7515 [==============================] - 4519s 601ms/step - loss: 12.6897 - val_loss: 12.6506\n",
      "Epoch 65/100\n",
      "7515/7515 [==============================] - 4536s 604ms/step - loss: 12.5472 - val_loss: 12.6325\n",
      "Epoch 66/100\n",
      "7515/7515 [==============================] - 4539s 604ms/step - loss: 12.4488 - val_loss: 12.4403\n",
      "Epoch 67/100\n",
      "7515/7515 [==============================] - 4544s 605ms/step - loss: 12.3517 - val_loss: 12.4582\n",
      "Epoch 68/100\n",
      "7515/7515 [==============================] - 4555s 606ms/step - loss: 12.2546 - val_loss: 12.4212\n",
      "Epoch 69/100\n",
      "7515/7515 [==============================] - 4557s 606ms/step - loss: 12.1710 - val_loss: 12.4042\n",
      "Epoch 70/100\n",
      "7515/7515 [==============================] - 4537s 604ms/step - loss: 12.1349 - val_loss: 12.3083\n",
      "Epoch 71/100\n",
      "7515/7515 [==============================] - 4537s 604ms/step - loss: 12.0410 - val_loss: 12.2431\n",
      "Epoch 72/100\n",
      "7515/7515 [==============================] - 4538s 604ms/step - loss: 11.9986 - val_loss: 12.3665\n",
      "Epoch 73/100\n",
      "7515/7515 [==============================] - 4540s 604ms/step - loss: 11.9572 - val_loss: 12.0496\n",
      "Epoch 74/100\n",
      "7515/7515 [==============================] - 4534s 603ms/step - loss: 11.8938 - val_loss: 12.0361\n",
      "Epoch 75/100\n",
      "7515/7515 [==============================] - 4532s 603ms/step - loss: 11.8355 - val_loss: 12.1782\n",
      "Epoch 76/100\n",
      "7515/7515 [==============================] - 4556s 606ms/step - loss: 11.8256 - val_loss: 12.0543\n",
      "Epoch 77/100\n",
      "7515/7515 [==============================] - 4534s 603ms/step - loss: 11.7566 - val_loss: 12.0726\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 78/100\n",
      "7515/7515 [==============================] - 4532s 603ms/step - loss: 11.0733 - val_loss: 11.2619\n",
      "Epoch 79/100\n",
      "7515/7515 [==============================] - 4532s 603ms/step - loss: 10.8419 - val_loss: 10.9819\n",
      "Epoch 80/100\n",
      "7515/7515 [==============================] - 4534s 603ms/step - loss: 10.7466 - val_loss: 10.9012\n",
      "Epoch 81/100\n",
      "7515/7515 [==============================] - 4534s 603ms/step - loss: 10.6567 - val_loss: 10.9087\n",
      "Epoch 82/100\n",
      "7515/7515 [==============================] - 4536s 604ms/step - loss: 10.5595 - val_loss: 10.7113\n",
      "Epoch 83/100\n",
      "7515/7515 [==============================] - 4558s 607ms/step - loss: 10.5243 - val_loss: 10.8845\n",
      "Epoch 84/100\n",
      "7515/7515 [==============================] - 4541s 604ms/step - loss: 10.5123 - val_loss: 10.7617\n",
      "Epoch 85/100\n",
      "7515/7515 [==============================] - 4538s 604ms/step - loss: 10.4215 - val_loss: 10.7599\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 86/100\n",
      "7515/7515 [==============================] - 4539s 604ms/step - loss: 10.3744 - val_loss: 10.5910\n",
      "Epoch 87/100\n",
      "7515/7515 [==============================] - 4544s 605ms/step - loss: 10.3683 - val_loss: 10.6628\n",
      "Epoch 88/100\n",
      "7515/7515 [==============================] - 4559s 607ms/step - loss: 10.3080 - val_loss: 10.5315\n",
      "Epoch 89/100\n",
      "7515/7515 [==============================] - 4578s 609ms/step - loss: 10.3304 - val_loss: 10.6367\n",
      "Epoch 90/100\n",
      "7515/7515 [==============================] - 4550s 606ms/step - loss: 10.3101 - val_loss: 10.6367\n",
      "Epoch 91/100\n",
      "7515/7515 [==============================] - 4579s 609ms/step - loss: 10.3002 - val_loss: 10.6774\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 92/100\n",
      "7515/7515 [==============================] - 4582s 610ms/step - loss: 10.2837 - val_loss: 10.5613\n",
      "Epoch 93/100\n",
      "7515/7515 [==============================] - 4573s 609ms/step - loss: 10.3098 - val_loss: 10.5825\n",
      "Epoch 94/100\n",
      "7515/7515 [==============================] - 4560s 607ms/step - loss: 10.3062 - val_loss: 10.6127\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "Epoch 95/100\n",
      "7515/7515 [==============================] - 4596s 612ms/step - loss: 10.2810 - val_loss: 10.5956\n",
      "Epoch 96/100\n",
      "7515/7515 [==============================] - 4553s 606ms/step - loss: 10.2900 - val_loss: 10.5725\n",
      "Epoch 97/100\n",
      "7515/7515 [==============================] - 4560s 607ms/step - loss: 10.2835 - val_loss: 10.5450\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
      "Epoch 98/100\n",
      "7515/7515 [==============================] - 4561s 607ms/step - loss: 10.2579 - val_loss: 10.6019\n",
      "Epoch 00098: early stopping\n"
     ]
    }
   ],
   "source": [
    "#K.clear_session()\n",
    "\n",
    "# Unfreeze and continue training, to fine-tune.\n",
    "    # Train longer if the result is not good.\n",
    "\n",
    "for i in range(len(model.layers)):\n",
    "    model.layers[i].trainable = True\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
    "print('Unfreeze all of the layers.')\n",
    "\n",
    "batch_size = 4 # note that more GPU memory is required after unfreezing the body\n",
    "print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "    steps_per_epoch=max(1, num_train//batch_size),\n",
    "    validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "    validation_steps=max(1, num_val//batch_size),\n",
    "    epochs=100,\n",
    "    initial_epoch=50,\n",
    "    callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n",
    "model.save_weights(log_dir + 'trained_weights_final.h5')\n",
    "\n",
    "    # Further training if needed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
